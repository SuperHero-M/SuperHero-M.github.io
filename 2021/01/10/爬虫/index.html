<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>爬虫 | super_M</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="爬虫抓取页面requests &#x2F; aiohttp &#x2F; httpx (抓取页面较为简单，所以就不细讲，只列出方法，不会可自行搜索学习，简单易上手) 解析页面正则表达式 —&gt;re首先需要导入re和requests模块 12import reimport requests  使用requests.get(‘网址’) ——向网站发送get请求 123456resp &#x3D; requests.g">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫">
<meta property="og:url" content="http://example.com/2021/01/10/%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="super_M">
<meta property="og:description" content="爬虫抓取页面requests &#x2F; aiohttp &#x2F; httpx (抓取页面较为简单，所以就不细讲，只列出方法，不会可自行搜索学习，简单易上手) 解析页面正则表达式 —&gt;re首先需要导入re和requests模块 12import reimport requests  使用requests.get(‘网址’) ——向网站发送get请求 123456resp &#x3D; requests.g">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-01-09T16:00:00.000Z">
<meta property="article:modified_time" content="2021-01-22T06:42:34.399Z">
<meta property="article:author" content="马琦添">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="super_M" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">super_M</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-爬虫" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/01/10/%E7%88%AC%E8%99%AB/" class="article-date">
  <time datetime="2021-01-09T16:00:00.000Z" itemprop="datePublished">2021-01-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Python%E8%BF%9B%E9%98%B6/">Python进阶</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      爬虫
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h1><h2 id="抓取页面"><a href="#抓取页面" class="headerlink" title="抓取页面"></a>抓取页面</h2><p>requests / aiohttp / httpx (抓取页面较为简单，所以就不细讲，只列出方法，不会可自行搜索学习，简单易上手)</p>
<h2 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h2><h3 id="正则表达式-—-gt-re"><a href="#正则表达式-—-gt-re" class="headerlink" title="正则表达式 —&gt;re"></a>正则表达式 —&gt;re</h3><p>首先需要导入re和requests模块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import requests</span><br></pre></td></tr></table></figure>

<p>使用requests.get(‘网址’) ——向网站发送get请求</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">resp &#x3D; requests.get(</span><br><span class="line">    url&#x3D;&#39;https:&#x2F;&#x2F;www.sohu.com&#39;,</span><br><span class="line">    headers&#x3D;&#123;</span><br><span class="line">        &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;87.0.4280.88 Safari&#x2F;537.36&#39;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>注意（如果无法请求成功可添加User-Agent和cookie以增大请求数据成功的概率）</p>
<p>cookie可在网页中鼠标右键检查选择network，然后刷新网页之后点击刷新项查看headers，就可以获取cookie值</p>
<p>请求成功之后就可使用正则表达式进行匹配标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 使用正则表达式捕获组从页面代码中提取电影名称</span><br><span class="line">patten &#x3D; re.compile(r&#39;\&lt;span class&#x3D;&quot;title&quot;\&gt;(.*?)\&lt;\&#x2F;span\&gt;&#39;)</span><br><span class="line">print(patten.findall(resp.text))</span><br></pre></td></tr></table></figure>

<p>注意：\为取消符号特殊意义，(.*?)为抓取目标</p>
<p>抓取成功之后保存到文件夹或者数据库即可</p>
<h3 id="css选择器-—-gt-beautifulsoup4"><a href="#css选择器-—-gt-beautifulsoup4" class="headerlink" title="css选择器 —&gt;beautifulsoup4"></a>css选择器 —&gt;beautifulsoup4</h3><p>想要使用css选择器首先得安装第三方库</p>
<p>在pycharm的terminal中输入pip install beautifulsoup4回车即可，显示安装成功后就可使用该第三方库</p>
<p>导入bs4模块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import bs4</span><br></pre></td></tr></table></figure>

<p>向网站发送请求一般选取requests，参考上面即可</p>
<p>请求成功之后，去找到网页的代码（网站中鼠标右键检查），根据代码的标签寻找自己想要爬取内容的标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup &#x3D; bs4.BeautifulSoup(resp.text, &#39;html.parser&#39;)</span><br><span class="line">spans &#x3D; soup.select(&#39;ul.li&gt;div5&gt;a&#39;)</span><br></pre></td></tr></table></figure>

<p>这里写的(‘ul.li&gt;div5&gt;a’)意思是ul标签下的li子标签，li标签下的第五个div下的a标签</p>
<p>抓取成功保存即可</p>
<h3 id="xpath-—-gt-从XML文件中获取元素的查询语法-—-gt-lxml"><a href="#xpath-—-gt-从XML文件中获取元素的查询语法-—-gt-lxml" class="headerlink" title="xpath —&gt;从XML文件中获取元素的查询语法 —&gt; lxml"></a>xpath —&gt;从XML文件中获取元素的查询语法 —&gt; lxml</h3><p>使用xpath想要安装第三方库lxml</p>
<p>terminal终端输入pip install lxml，显示安装成功后就可使用该第三方库</p>
<p>导入lxml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from lxml import etree</span><br></pre></td></tr></table></figure>

<p>使用requests向网站发送请求，成功后</p>
<p>去网站找到html代码，选中自己要获取的内容，鼠标右键，找到copy，找到copy xpath点击</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root &#x3D; etree.HTML(resp.text)</span><br><span class="line">spans &#x3D; root.xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;content&quot;]&#x2F;div&#x2F;div[1]&#x2F;ol&#x2F;li&#x2F;div&#x2F;div[2]&#x2F;div[1]&#x2F;a&#x2F;span[1]&#39;)</span><br></pre></td></tr></table></figure>

<p>注意：粘贴的xpth格式只有当前选中的一行数据，可根据自己需要的数据对代码进行修改，即可拿到想要的代码</p>
<p>例如去掉div[2]中的[2]</p>
<p>拿到数据下载保存即可</p>
<h2 id="保存数据（数据持久化）"><a href="#保存数据（数据持久化）" class="headerlink" title="保存数据（数据持久化）"></a>保存数据（数据持久化）</h2><p>上面讲了如何爬取数据，这里将会告诉大家如何保存数据</p>
<h3 id="Execl"><a href="#Execl" class="headerlink" title="Execl"></a>Execl</h3><p>该库是保存到表格中</p>
<p>安装第三方库 pip install xlwt openpyxl</p>
<p>导入模块xlwt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import xlwt</span><br></pre></td></tr></table></figure>

<p>下面给到家参考一份我写的完整的保存数据到Excel中的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">    # 创建工作簿</span><br><span class="line">    wb &#x3D; xlwt.Workbook()</span><br><span class="line">    # 创建工作表</span><br><span class="line">    sheet &#x3D; wb.add_sheet(&#39;Top250&#39;)</span><br><span class="line">    col_names &#x3D; (&#39;排名&#39;, &#39;名称&#39;, &#39;评分&#39;, &#39;类型&#39;, &#39;制片国家&#39;, &#39;语言&#39;, &#39;时长&#39;)</span><br><span class="line">    for index, name in enumerate(col_names):</span><br><span class="line">        sheet.write(0, index, name)</span><br><span class="line">        rank &#x3D; 0</span><br><span class="line">    for page in range(10):</span><br><span class="line">        resp &#x3D; requests.get(</span><br><span class="line">            url&#x3D;f&#39;https:&#x2F;&#x2F;movie.douban.com&#x2F;top250?start&#x3D;&#123;page * 25&#125;&#39;,</span><br><span class="line">            headers&#x3D;&#123;</span><br><span class="line">                &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;87.0.4280.88 Safari&#x2F;537.36&#39;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        soup &#x3D; bs4.BeautifulSoup(resp.text, &#39;html.parser&#39;)</span><br><span class="line">        info_divs &#x3D; soup.select(&#39;div.info&#39;)</span><br><span class="line">        for info_div in info_divs:</span><br><span class="line">            rank +&#x3D; 1</span><br><span class="line">            anchor &#x3D; info_div.select_one(&#39;div.hd&gt;a&#39;)</span><br><span class="line">            detail_url &#x3D; anchor.attrs[&#39;href&#39;]</span><br><span class="line">            title &#x3D; anchor.select_one(&#39;span.title&#39;).text</span><br><span class="line">            score &#x3D; info_div.select_one(&#39;div.bd&gt;div.star&gt;span.rating_num&#39;).text</span><br><span class="line">            movie_details &#x3D; [rank, title, score]</span><br><span class="line">            movie_details +&#x3D; fetch_movie_detail(detail_url)</span><br><span class="line">            for index, item in enumerate(movie_details):</span><br><span class="line">                sheet.write(rank, index, item)</span><br><span class="line">        wb.save(&#39;豆瓣电影.xls&#39;)</span><br><span class="line"></span><br><span class="line">    print(&#39;程序结束&#39;)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h3 id="CSV-—-gt-reader-writer-file"><a href="#CSV-—-gt-reader-writer-file" class="headerlink" title="CSV —&gt;reader / writer(file)"></a>CSV —&gt;reader / writer(file)</h3><p>CSV操作我给大家参考一份下载图片的代码，由于好理解，我就不详细给大家解释了，大家根据代码参考就可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">resp &#x3D; requests.get(&#39;https:&#x2F;&#x2F;bkimg.cdn.bcebos.com&#x2F;pic&#x2F;562c11dfa9ec8a1363271293564a868fa0ec08fa6a24?x-bce-process&#x3D;image&#x2F;watermark,image_d2F0ZXIvYmFpa2UxMTY&#x3D;,g_7,xp_5,yp_5&#39;)</span><br><span class="line">with open(&#39;sanji.png&#39;, &#39;wb&#39;) as file:</span><br><span class="line">    file.write(resp.content)</span><br></pre></td></tr></table></figure>

<p>也可将数据保存到数据库当中，这种方法一般企业使用更多，队友新手党来说，学会上面两种方法即可</p>
<h3 id="数据库（Database）"><a href="#数据库（Database）" class="headerlink" title="数据库（Database）"></a>数据库（Database）</h3><p>关系型数据库（SQL）<br>库（NoSQL —&gt; NewSQL）</p>
<h2 id="并发爬取"><a href="#并发爬取" class="headerlink" title="并发爬取"></a>并发爬取</h2><p>由于爬取方式是一个一个下载，可能速度会相对较慢，我们就可以使用多线程或者多进程爬取，也可使用线程池或者进程池</p>
<h3 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h3><p>多线程是由一个进程中的多个线程组成，多个线程可以同时进行文件的下载，适用于下载文件之间没有先后顺序的下载</p>
<p>多线程之间通信比较简单，因为线程可以共享进程的内容</p>
<p>使用多线程需要导入模块Thread</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from threading import Thread</span><br></pre></td></tr></table></figure>

<p>这里我用代码向大家演示多线程的使用方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def record_time(func):</span><br><span class="line"></span><br><span class="line">    def wrapper(*args, **kwargs):</span><br><span class="line">        start &#x3D; time.time()</span><br><span class="line">        result &#x3D; func(*args, **kwargs)</span><br><span class="line">        end &#x3D; time.time()</span><br><span class="line">        print(f&#39;&#123;func.__name__&#125;执行时间:&#123;end - start:.3f&#125;秒&#39;)</span><br><span class="line">        return result</span><br><span class="line"></span><br><span class="line">    return wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@record_time</span><br><span class="line">def download(filename):</span><br><span class="line">    print(f&#39;开始下载&#123;filename&#125;&#39;)</span><br><span class="line">    time.sleep(random.randint(5, 10))</span><br><span class="line">    print(f&#39;&#123;filename&#125;下载完成&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@record_time</span><br><span class="line">def upload(filename):</span><br><span class="line">    print(f&#39;开始上传&#123;filename&#125;&#39;)</span><br><span class="line">    time.sleep(random.randint(5, 10))</span><br><span class="line">    print(f&#39;&#123;filename&#125;上传完成&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 如果程序里有好时间的任务，可能会阻塞别的任务，就用线程来执行，如果有因果关系就必须等着，按顺序执行，如果没有因果关系，就可以用线程</span><br><span class="line">@record_time</span><br><span class="line">def main():</span><br><span class="line">    t1 &#x3D; Thread(target&#x3D;download, args&#x3D;(&#39;Python从入门到住院.pdf&#39;, ))</span><br><span class="line">    t1.start()</span><br><span class="line">    t2 &#x3D; Thread(target&#x3D;download, args&#x3D;(&#39;MySQL从删库到跑路.avi&#39;, ))</span><br><span class="line">    t2.start()</span><br><span class="line">    t3 &#x3D; Thread(target&#x3D;upload, args&#x3D;(&#39;北京有点热.avi&#39;, ))</span><br><span class="line">    t3.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line">    t3.join()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>注意：args后面为元组，所以需要在最后打逗号</p>
<p>这里我使用了一个装饰器，用来装饰下面三个函数</p>
<h3 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h3><p>使用线程池需要导入模块ThreadPoolExecutor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from concurrent.futures.thread import ThreadPoolExecutor</span><br></pre></td></tr></table></figure>

<p>使用方法我之间用代码向大家演示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ef download_picture(picture_url):</span><br><span class="line">    resp &#x3D; requests.get(picture_url)</span><br><span class="line">    if resp.status_code &#x3D;&#x3D; 200:</span><br><span class="line">        filename &#x3D; picture_url[picture_url.rfind(&#39;&#x2F;&#39;) + 1:]</span><br><span class="line">        with open(f&#39;images&#x2F;&#123;filename&#125;&#39;, &#39;wb&#39;) as file:</span><br><span class="line">            file.write(resp.content)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">with ThreadPoolExecutor(max_workers&#x3D;16) as pool:</span><br><span class="line">    for num in range(1, 11):</span><br><span class="line">        resp &#x3D; requests.get(&#39;https:&#x2F;&#x2F;image.so.com&#x2F;zjl?ch&#x3D;beauty&amp;t1&#x3D;600&amp;sn&#x3D;30&#39;)</span><br><span class="line">        data_dict &#x3D; resp.json()</span><br><span class="line">        for beauty_dict in data_dict[&#39;list&#39;]:</span><br><span class="line">            picture_url &#x3D; beauty_dict[&#39;qhimg_url&#39;]</span><br><span class="line">            pool.submit(download_picture, picture_url)</span><br></pre></td></tr></table></figure>

<h3 id="多线程和多进程"><a href="#多线程和多进程" class="headerlink" title="多线程和多进程"></a>多线程和多进程</h3><p>使用方法和多线程，线程池差不多</p>
<p>将Tread替换为Process，ThreadPoolExecutor替换为ProcessPoolExecutor即可</p>
<h3 id="异步编程"><a href="#异步编程" class="headerlink" title="异步编程"></a>异步编程</h3><p>异步不需要排队，没有顺序，不会阻塞，也不会等待</p>
<p>需要模块aiohttp和asyncio</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import aiohttp</span><br><span class="line">import asyncio</span><br></pre></td></tr></table></figure>

<p>直接代码展示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">U_A &#x3D; &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;87.0.4280.141 Safari&#x2F;537.36&#39;</span><br><span class="line"></span><br><span class="line">urls &#x3D; [</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.bilibili.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.sohu.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.taobao.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.jd.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.qidian.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.huya.com&#x2F;&#39;,</span><br><span class="line">    &#39;https:&#x2F;&#x2F;www.douyu.com&#x2F;&#39;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">title_pattern &#x3D; re.compile(r&#39;\&lt;title\&gt;(?P&lt;foo&gt;.*?)\&lt;\&#x2F;title\&gt;&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">async def main(url):</span><br><span class="line">    async with aiohttp.ClientSession() as session:</span><br><span class="line">        async with session.get(url, headers&#x3D;&#123;&#39;User-Agent&#39;: U_A&#125;) as resp:</span><br><span class="line">            page_code &#x3D; await resp.text()</span><br><span class="line">            match &#x3D; title_pattern.search(page_code)</span><br><span class="line">            if match:</span><br><span class="line">                print(match.group(&#39;foo&#39;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cos_list &#x3D; [main(url) for url in urls]</span><br><span class="line">loop &#x3D; asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(cos_list))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/10/%E7%88%AC%E8%99%AB/" data-id="ckk7xyzon000593qg413i3srp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/11/13/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%9F%BA%E7%A1%80/">Python基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E8%BF%9B%E9%98%B6/">Python进阶</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/01/10/%E7%88%AC%E8%99%AB/">爬虫</a>
          </li>
        
          <li>
            <a href="/2020/11/13/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2020/09/01/Python%E5%8F%82%E8%80%83%E4%B9%A6%E7%B1%8D/">Python参考书籍</a>
          </li>
        
          <li>
            <a href="/2020/08/01/Python%E4%B9%8B%E7%A6%85%E7%9A%84%E6%9C%80%E4%BD%B3%E7%BF%BB%E8%AF%91/">Python之禅</a>
          </li>
        
          <li>
            <a href="/2019/08/01/Python%E7%BC%96%E7%A8%8B%E6%83%AF%E4%BE%8B/">Python编程惯例</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 马琦添<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>